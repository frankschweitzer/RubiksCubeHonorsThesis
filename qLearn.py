# -*- coding: utf-8 -*-
"""cube_q_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKRApQDFSTQka0rV8Wa6GsPWsJhhNHEa

**Imports**
"""

# !pip install rubik-cube

from rubik.cube import Cube
import numpy as np
import random
import json

"""**Cube Environment**"""


# for the reset of the environment
def scramble_cube(n):
    c = Cube('WWWWWWWWWOOOGGGRRRBBBOOOGGGRRRBBBOOOGGGRRRBBBYYYYYYYYY')
    scramble_moves = " ".join(random.choices(
        ['Ri', 'Li', 'Ui', 'Di', 'Fi', 'Bi'], k=n))
    c.sequence(scramble_moves)
    return c

# determines if the current state is solved
def solved(c: Cube):
    return c.is_solved()


# rewards agent for solving / getting closer to solving cube
def reward(c: Cube):
    if c.is_solved():
        return 100

    flattened_cube = c.flat_str()

    U = flattened_cube[0:9]
    L = flattened_cube[9:12] + flattened_cube[21:24] + flattened_cube[33:36]
    F = flattened_cube[12:15] + flattened_cube[24:27] + flattened_cube[36:39]
    R = flattened_cube[15:18] + flattened_cube[27:30] + flattened_cube[39:42]
    B = flattened_cube[18:21] + flattened_cube[30:33] + flattened_cube[42:45]
    D = flattened_cube[45:54]

    U_point = all(char == U[0] for char in U)
    L_point = all(char == L[0] for char in L)
    F_point = all(char == F[0] for char in F)
    R_point = all(char == R[0] for char in R)
    B_point = all(char == B[0] for char in B)
    D_point = all(char == D[0] for char in D)

    reward = 0
    if U_point:
        reward += 1
    if L_point:
        reward += 1
    if F_point:
        reward += 1
    if R_point:
        reward += 1
    if B_point:
        reward += 1
    if D_point:
        reward += 1

    return reward


# moves the cube with the provided action
def move(c: Cube, action):
    c.sequence(action)
    r = reward(c)
    d = solved(c)
    return c, r, d


"""**Agent Training**"""

# choose action with epsilon greedy policy
def choose_action(q_table, state, epsilon):
    if random.uniform(0, 1) > epsilon:
        return np.argmax(q_table.get(state))
    else:
        return random.randint(0, 11)


def train_loop(episodes=30000000, min_eps=0.1, decay_rate=0.999, max_eps=1, max_moves=20, gamma=.95, lr=0.6):
    print(f'Training for {episodes} episodes, decay rate of {decay_rate}, max moves of {max_moves}, gamma of {gamma}, and lr of {lr}')
    q_table = {}
    epsilon = max_eps
    action_map = {0: 'R', 1: 'L', 2: 'U', 3: 'D', 4: 'F', 5: 'B',
                  6: 'Ri', 7: 'Li', 8: 'Ui', 9: 'Di', 10: 'Fi', 11: 'Bi'}
    rand_scrambles = 1

    for episode in range(episodes):
        # initialize new episode with random cube
        if episode < 500:
            rand_scrambles = 1
        elif episode >= 500 and episode < 1500:
            rand_scrambles = 2
        elif episode >= 1500 and episode < 6000:
            rand_scrambles = 3
        elif episode >= 6000 and episode < 30000:
            rand_scrambles = 4
        elif episode >= 30000 and episode < 60000:
            rand_scrambles = 5
        elif episode >= 60000 and episode < 200000:
            rand_scrambles = 6
        elif episode <= 200000 and episode < 1000000:
            rand_scrambles = 7
        else:
            rand_scrambles = random.randint(1,20)
        state = scramble_cube(rand_scrambles)
        total_reward = 0
        done = False

        for m in range(max_moves):
            # obtain an action
            curr_state = Cube(state)
            action_num = choose_action(q_table, state.flat_str(), epsilon)
            action = action_map[action_num]

            # observe action and consequence
            next_state, reward, done = move(state, action)

            # update q values with bellman equation
            curr_q = q_table.get(curr_state.flat_str(),
                                 np.zeros(12))[action_num]
            next_q = np.max(q_table.get(next_state.flat_str(), np.zeros(12)))

            q_table.setdefault(curr_state.flat_str(), np.zeros(12))[
                action_num] = curr_q + lr * (reward + gamma * next_q - curr_q)

            # update reward and epsilon via epsilon annealing
            total_reward += reward
            if epsilon > min_eps:
                epsilon *= decay_rate

            if done:
                break

            state = next_state

        print(f"Episode {episode+1} total reward {total_reward}")

    return q_table


learned_q_table = train_loop()

# save q table to file
for scramble, q_values in learned_q_table.items():
    learned_q_table[scramble] = q_values.tolist()
with open('q_table.json', 'w') as file:
    json.dump(learned_q_table, file)

"""**Testing**"""

move_map = {0: 'R', 1: 'L', 2: 'U', 3: 'D', 4: 'F', 5: 'B',
            6: 'Ri', 7: 'Li', 8: 'Ui', 9: 'Di', 10: 'Fi', 11: 'Bi'}
iterations = 5000

for scr in range(1, 20):
    avg_moves = 0
    cubes_solved = 0

    for n in range(iterations):
        test_cube = scramble_cube(scr)
        orig_cube = Cube(test_cube)

        i = 0
        moves = []
        while not test_cube.is_solved() and i < 20:
            prediction = move_map[np.argmax(
                np.array(learned_q_table.get(test_cube.flat_str())))]
            test_cube.sequence(prediction)
            moves.append(prediction)

            i += 1

        if test_cube.is_solved():
            if cubes_solved < 10 and scr > 3:
                print(f'Cube of {scr} scrambles was solved!')
                print(orig_cube)
                print(moves)
            cubes_solved += 1
            avg_moves += i

    avg_moves = avg_moves / cubes_solved
    print(f"{cubes_solved} cubes were solved out of {iterations} iterations for scramble of {scr}")
    print(f"the average number of moves to solved the cubes was {avg_moves}")
