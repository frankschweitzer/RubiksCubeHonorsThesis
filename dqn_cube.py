# -*- coding: utf-8 -*-
"""DQN_Cube.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qq91P3_PPC4KVO_7hdh72t1p95dV6Ccf
"""

# !pip freeze

# !pip install rubik-cube

"""**Imports**"""

from rubik.cube import Cube
import numpy as np
import tensorflow as tf
import random
from collections import deque

"""**Neural Network Class**"""

# create NN
class NN:
    # initialize the model
    def __init__(self, state_size, action_size, learning_rate):
        self.model = self._build_model(state_size, action_size, learning_rate)

    def _build_model(self, state_size, action_size, learning_rate):
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Dense(64, input_dim=state_size, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.2))
        model.add(tf.keras.layers.Dense(32, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.2))
        model.add(tf.keras.layers.Dense(16, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.2))
        model.add(tf.keras.layers.Dense(action_size))
        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))
        return model

"""**Memory Replay Class**"""

# create replay memory
class ReplayMemory:
    # maxlen removes oldest experience once full
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    # add an experience
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    # obtain random samples of experiences
    def sample(self, batch_size):
        sampled_batch = random.sample(self.buffer, batch_size)

        states, actions, rewards, next_states, dones = [], [], [], [], []
        for experience in sampled_batch:
            state, action, reward, next_state, done = experience
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            next_states.append(next_state)
            dones.append(done)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)

"""**Greedy Epsilon to Determine Action**"""

# epsilon greedy with annealing
def epsilon_greedy_action(model, state, epsilon):
    if random.random() < epsilon:
      # take a random action
      act = random.randint(0,5)
      return act
    else:
      # use online network suggested action
      Q = model.predict(preprocess([state]))
      return np.argmax(Q)

"""**Cube Environment**"""

# for the reset of the environment
def scramble_cube(num):
  c = Cube('WWWWWWWWWOOOGGGRRRBBBOOOGGGRRRBBBOOOGGGRRRBBBYYYYYYYYY')
  scramble_moves = " ".join(random.choices(['Ri','Li','Ui','Di','Fi','Bi'], k=num))
  c.sequence(scramble_moves)
  return c

# determines if the current state is solved
def solved(c: Cube):
  return c.is_solved()

# correct color reward system
# def reward(c: Cube):
#   # extract layout of cube
#   cube_str = c.flat_str()
#   U = cube_str[0:9]
#   L = cube_str[9:12] + cube_str[21:24] + cube_str[33:36]
#   F = cube_str[12:15] + cube_str[24:27] + cube_str[36:39]
#   R = cube_str[15:18] + cube_str[27:30] + cube_str[39:42]
#   B = cube_str[18:21] + cube_str[30:33] + cube_str[42:45]
#   D = cube_str[45:54]

#   # get center colors
#   U_center, L_center, F_center, R_center, B_center, D_center = U[4], L[4], F[4], R[4], B[4], D[4]

#   # get color similarity counts
#   r = ((U.count(U_center)-1) +
#             (L.count(L_center)-1) +
#             (F.count(F_center)-1) +
#             (R.count(R_center)-1) +
#             (B.count(B_center)-1) +
#             (D.count(D_center)-1))

#   return r

# 0 1 reward system
# def reward(c: Cube):
#   if c.is_solved():
#     return 1
#   return 0

# solved side reward system
def reward(c: Cube):
  if c.is_solved():
    return 10

  flattened_cube = c.flat_str()

  U = flattened_cube[0:9]
  L = flattened_cube[9:12] + flattened_cube[21:24] + flattened_cube[33:36]
  F = flattened_cube[12:15] + flattened_cube[24:27] + flattened_cube[36:39]
  R = flattened_cube[15:18] + flattened_cube[27:30] + flattened_cube[39:42]
  B = flattened_cube[18:21] + flattened_cube[30:33] + flattened_cube[42:45]
  D = flattened_cube[45:54]

  U_point = all(char == U[0] for char in U)
  L_point = all(char == L[0] for char in L)
  F_point = all(char == F[0] for char in F)
  R_point = all(char == R[0] for char in R)
  B_point = all(char == B[0] for char in B)
  D_point = all(char == D[0] for char in D)

  reward = 0
  if U_point:
    reward += 1
  if L_point:
    reward += 1
  if F_point:
    reward += 1
  if R_point:
    reward += 1
  if B_point:
    reward += 1
  if D_point:
    reward += 1

  return reward

# experience
def move(c: Cube, action):
  c.sequence(action)
  r = reward(c)
  d = solved(c)
  return c, r, d

# vectorize cube
def preprocess(cubes):
  preprocessed_cubes = []
  for c in cubes:
    cube_str = c.flat_str()
    cube_vec = np.array(list(cube_str))
    # assign colors a numerical value
    cube_vec[cube_vec == 'W'] = 1
    cube_vec[cube_vec == 'Y'] = 2
    cube_vec[cube_vec == 'R'] = 3
    cube_vec[cube_vec == 'B'] = 4
    cube_vec[cube_vec == 'G'] = 5
    cube_vec[cube_vec == 'O'] = 6
    cube_vec = cube_vec.astype(float)
    # normalize by dividing by the sum of the vector
    cube_vec_normalized = cube_vec / np.sum(cube_vec)
    cube_vec_normalized.reshape(-1,1)
    preprocessed_cubes.append(cube_vec_normalized)
  return np.array(preprocessed_cubes)

"""**Training**"""

def train_dqn(num_episodes=10000, batch_size=32, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.999, target_update=10, lr=1e-3, capacity=256):

    # state size is 54 for all of the stickers
    input_size = 54
    # number of cube actions {F, B, U, D, R, L}
    output_size = 6

    action_map = {0: 'R', 1: 'L', 2: 'U', 3: 'D', 4: 'F', 5: 'B'}
    rev_action_map = {'R':0, 'L':1, 'U':2, 'D':3, 'F':4, 'B':5}
    epsilon = epsilon_start

    # initialize online and target networks with random weights
    online_network = NN(input_size, output_size, learning_rate=lr)
    target_network = NN(input_size, output_size, learning_rate=lr)
    # set target network with same parameters as online network
    target_network.model.set_weights(online_network.model.get_weights())

    # initialize the replay memory
    replay_memory = ReplayMemory(capacity)

    for episode in range(num_episodes):

        # initialize a new state
        state = scramble_cube(3)
        orig_state = Cube(state)
        done = False
        total_reward = 0

        i, max_moves = 0, 5
        # while not done:
        while i < max_moves:
            # choose an action using epsilon greedy policy
            action = epsilon_greedy_action(online_network.model, state, epsilon)
            action = action_map[action]

            # execute action and observe experience
            curr_state = Cube(state)
            next_state, reward, done = move(state, action)
            total_reward += reward

            # if cube is complete move on to next episode
            if done:
              print("ORIG STATE")
              print(orig_state)
              print("CURR STATE")
              print(curr_state)
              print("ACTION")
              print(action)
              print("NEXT STATE")
              print(next_state)
              break

            # store experience in replay memory and move to next state
            replay_memory.push(curr_state, action, reward, next_state, done)
            state = next_state
            i += 1

            if len(replay_memory) > batch_size:
              with tf.GradientTape() as tape:
                  # get a set of random experiences from replay memory
                  batch_state, batch_action, batch_reward, batch_next_state, batch_done = replay_memory.sample(batch_size)

                  # preprocess batches
                  preprocessed_bs = preprocess(batch_state)
                  preprocessed_bns = preprocess(batch_next_state)

                  # compute q (predicted) using online network and q* using target network
                  q1_value = online_network.model(preprocessed_bs)
                  q2_value = target_network.model(preprocessed_bns)

                  # compute target q value using Bellman equation
                  batch_reward_tensor = tf.convert_to_tensor(batch_reward, dtype=tf.float32)
                  batch_done_tensor = tf.convert_to_tensor(batch_done, dtype=tf.float32)

                  q2_value_max = tf.reduce_max(q2_value, axis=1)

                  target_q_value = batch_reward_tensor + (1 - batch_done_tensor) * gamma * q2_value_max

                  # compute loss of predicted and target q values
                  batch_action_indices = [rev_action_map[action] for action in batch_action]
                  batch_action_tensor = tf.convert_to_tensor(batch_action_indices, dtype=tf.int32)
                  q1_values_for_actions = tf.reduce_sum(q1_value * tf.one_hot(batch_action_tensor, depth=output_size), axis=1)
                  loss = tf.reduce_mean((target_q_value - q1_values_for_actions) ** 2)
                  # q1_max = tf.reduce_max(q1_value)
                  # loss = tf.reduce_mean((target_q_value - q1_max) ** 2)

                  # compute gradients of loss function
                  grads = tape.gradient(loss, online_network.model.trainable_variables)

                  # update parameters of online network
                  online_network.model.optimizer.apply_gradients(zip(grads, online_network.model.trainable_variables))

                  # epsilon annealing
                  if epsilon > epsilon_end:
                    epsilon *= epsilon_decay

        # on an interval copy the weights of online network to target network
        if episode % target_update == 0:
          target_network.model.set_weights(online_network.model.get_weights())

        print(f"Episode {episode+1}, Total Reward: {total_reward}, Epsilon: {epsilon}")

    return target_network

trained_network = train_dqn()

"""**Testing Model**"""

move_map = {0: 'R', 1: 'L', 2: 'U', 3: 'D', 4: 'F', 5: 'B'}

testing_cube = Cube('WWWWWWWWWOOOGGGRRRBBBOOOGGGRRRBBBOOOGGGRRRBBBYYYYYYYYY')
print(testing_cube)
testing_cube.sequence('U')

i = 1
while not testing_cube.is_solved():
  print(f'Iteration {i}')
  print(testing_cube)
  test_cubes = preprocess([testing_cube])
  print(test_cubes)

  predictions = trained_network.model(test_cubes)
  print(predictions)

  predicted_move = tf.argmax(predictions, axis=1).numpy()[0]
  print(move_map[predicted_move])

  testing_cube.sequence(move_map[predicted_move])

  i += 1

print('Cube Solved!')

