# -*- coding: utf-8 -*-
"""cube_q_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKRApQDFSTQka0rV8Wa6GsPWsJhhNHEa

**Imports**
"""

# !pip install rubik-cube

from rubik.cube import Cube
import numpy as np
import random

"""**Cube Environment**"""

# for the reset of the environment


def scramble_cube(n):
    c = Cube('WWWWWWWWWOOOGGGRRRBBBOOOGGGRRRBBBOOOGGGRRRBBBYYYYYYYYY')
    scramble_moves = " ".join(random.choices(
        ['Ri', 'Li', 'Ui', 'Di', 'Fi', 'Bi'], k=n))
    c.sequence(scramble_moves)
    return c

# determines if the current state is solved


def solved(c: Cube):
    return c.is_solved()

# correct color reward system
# def reward(c: Cube):
#   # extract layout of cube
#   cube_str = c.flat_str()
#   U = cube_str[0:9]
#   L = cube_str[9:12] + cube_str[21:24] + cube_str[33:36]
#   F = cube_str[12:15] + cube_str[24:27] + cube_str[36:39]
#   R = cube_str[15:18] + cube_str[27:30] + cube_str[39:42]
#   B = cube_str[18:21] + cube_str[30:33] + cube_str[42:45]
#   D = cube_str[45:54]

#   # get center colors
#   U_center, L_center, F_center, R_center, B_center, D_center = U[4], L[4], F[4], R[4], B[4], D[4]

#   # get color similarity counts
#   r = ((U.count(U_center)-1) +
#             (L.count(L_center)-1) +
#             (F.count(F_center)-1) +
#             (R.count(R_center)-1) +
#             (B.count(B_center)-1) +
#             (D.count(D_center)-1))

#   return r

# solved side reward system


def reward(c: Cube):
    if c.is_solved():
        return 100

    flattened_cube = c.flat_str()

    U = flattened_cube[0:9]
    L = flattened_cube[9:12] + flattened_cube[21:24] + flattened_cube[33:36]
    F = flattened_cube[12:15] + flattened_cube[24:27] + flattened_cube[36:39]
    R = flattened_cube[15:18] + flattened_cube[27:30] + flattened_cube[39:42]
    B = flattened_cube[18:21] + flattened_cube[30:33] + flattened_cube[42:45]
    D = flattened_cube[45:54]

    U_point = all(char == U[0] for char in U)
    L_point = all(char == L[0] for char in L)
    F_point = all(char == F[0] for char in F)
    R_point = all(char == R[0] for char in R)
    B_point = all(char == B[0] for char in B)
    D_point = all(char == D[0] for char in D)

    reward = 0
    if U_point:
        reward += 1
    if L_point:
        reward += 1
    if F_point:
        reward += 1
    if R_point:
        reward += 1
    if B_point:
        reward += 1
    if D_point:
        reward += 1

    return reward

# experience


def move(c: Cube, action):
    c.sequence(action)
    r = reward(c)
    d = solved(c)
    return c, r, d


"""**Agent Training**"""


def choose_action(q_table, state, epsilon):
    if random.uniform(0, 1) > epsilon:
        return np.argmax(q_table.get(state))
    else:
        return random.randint(0, 11)


def train_loop(episodes=500000, min_eps=0.1, decay_rate=0.999, max_eps=1, max_moves=8, gamma=.95, lr=0.5):
    q_table = {}
    epsilon = max_eps
    action_map = {0: 'R', 1: 'L', 2: 'U', 3: 'D', 4: 'F', 5: 'B',
                  6: 'Ri', 7: 'Li', 8: 'Ui', 9: 'Di', 10: 'Fi', 11: 'Bi'}
    rand_scrambles = 1

    for episode in range(episodes):
        # initialize new episode with random cube
        if episode < 500:
            rand_scrambles = 1
        elif episode >= 500 and episode < 1500:
            rand_scrambles = 2
        elif episode >= 1500 and episode < 6000:
            rand_scrambles = 3
        elif episode >= 6000 and episode < 30000:
            rand_scrambles = 4
        elif episode >= 30000 and episode < 60000:
            rand_scrambles = 5
        else:
          rand_scrambles = 6
        state = scramble_cube(rand_scrambles)
        total_reward = 0
        done = False

        for m in range(max_moves):
            # obtain an action
            curr_state = Cube(state)
            action_num = choose_action(q_table, state.flat_str(), epsilon)
            action = action_map[action_num]

            # observe action and consequence
            next_state, reward, done = move(state, action)

            # update q values with bellman equation
            curr_q = q_table.get(curr_state.flat_str(),
                                 np.zeros(12))[action_num]
            next_q = np.max(q_table.get(next_state.flat_str(), np.zeros(12)))

            q_table.setdefault(curr_state.flat_str(), np.zeros(12))[
                action_num] = curr_q + lr * (reward + gamma * next_q - curr_q)

            # update reward and epsilon via epsilon annealing
            total_reward += reward
            if epsilon > min_eps:
                epsilon *= decay_rate

            if done:
                break

            state = next_state

        print(f"Episode {episode+1} total reward {total_reward}")

    return q_table


learned_q_table = train_loop()

file = open('q_table.txt', 'w')
file.write(str(learned_q_table))
file.close()

"""**Testing**"""

move_map = {0: 'R', 1: 'L', 2: 'U', 3: 'D', 4: 'F', 5: 'B',
            6: 'Ri', 7: 'Li', 8: 'Ui', 9: 'Di', 10: 'Fi', 11: 'Bi'}
iterations = 5000

for scr in range(1, 5):
    avg_moves = 0
    cubes_solved = 0

    for n in range(iterations):
        test_cube = scramble_cube(scr)
        orig_cube = Cube(test_cube)

        i = 0
        while not test_cube.is_solved() and i < 10:
            prediction = move_map[np.argmax(
                np.array(learned_q_table.get(test_cube.flat_str())))]
            test_cube.sequence(prediction)

            i += 1

        if test_cube.is_solved():
            cubes_solved += 1
            avg_moves += i

    avg_moves = avg_moves / cubes_solved
    print(f"{cubes_solved} cubes were solved out of {iterations} iterations for scramble of {scr}")
    print(f"the average number of moves to solved the cubes was {avg_moves}")
